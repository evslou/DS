{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2. Классификация изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в есть датасете. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "Структура датасета простая -- есть директории train/ и val/, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат, собственно, сами изображения.\n",
    "\n",
    "__Задание__. Необходимо выполнить два задания\n",
    "\n",
    "1) Добейтесь accuracy **на валидации не менее 0.44**. В этом задании **запрещено** пользоваться предобученными моделями и ресайзом картинок. 5 баллов\n",
    "\n",
    "2) Добейтесь accuracy **на валидации не менее 0.84**. В этом задании делать ресайз и использовать претрейн можно. 5 баллов\n",
    "\n",
    "Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи / блогпосты / вопросы на stackoverflow / видосы от ютуберов-машинлернеров / курсы / подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете.\n",
    "\n",
    "Ваш код обязательно должен проходить все `assert`'ы ниже.\n",
    "\n",
    "__Использовать внешние данные для обучения строго запрещено в обоих заданиях. Также запрещено обучаться на валидационной выборке__.\n",
    "\n",
    "\n",
    "__Критерии оценки__: Оценка вычисляется по простой формуле: `min(10, 10 * Ваша accuracy / 0.44)` для первого задания и `min(10, 10 * (Ваша accuracy - 0.5) / 0.34)` для второго. Оценка округляется до десятых по арифметическим правилам.\n",
    "\n",
    "\n",
    "__Советы и указания__:\n",
    " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать :)\n",
    " - Используйте аугментации. Для этого пользуйтесь модулем `torchvision.transforms` или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - Можно обучать с нуля или файнтюнить (в зависимости от задания) модели из `torchvision`.\n",
    " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом `ImageFolder`), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
    " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
    " - Чтобы быстро отладить код, пробуйте обучаться на маленькой части датасета (скажем, 5-10 картинок просто чтобы убедиться что код запускается). Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
    " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
    " - Фиксируйте random seed.\n",
    " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
    " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
    " - Советуем использовать GPU. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
    "\n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.34.0-py2.py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.34.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n",
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-2.1.1-py3-none-any.whl (776 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.9.0 pytorch_lightning-2.1.1 torchmetrics-1.2.0\n",
      "Collecting lion_pytorch\n",
      "  Downloading lion_pytorch-0.1.2-py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from lion_pytorch) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->lion_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->lion_pytorch) (1.3.0)\n",
      "Installing collected packages: lion_pytorch\n",
      "Successfully installed lion_pytorch-0.1.2\n",
      "--2023-11-12 19:56:34--  https://www.dropbox.com/s/33l8lp62rmvtx40/dataset.zip?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.13.18, 2620:100:6016:18::a27d:112\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.13.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/dl/33l8lp62rmvtx40/dataset.zip [following]\n",
      "--2023-11-12 19:56:35--  https://www.dropbox.com/s/dl/33l8lp62rmvtx40/dataset.zip\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc349d8f9c0125e0b9e0bca55f5e.dl.dropboxusercontent.com/cd/0/get/CHYluIWEANvzu7UTaq26J9HHNDkz7LVj-Idr_Upu5wq4dGyKaPMUI8WR4MIySXwnBEej5oavWoLuzE_zV-dxjqBjFvDHUN9AeJLonGR0DyLm2O7e2s01OzzgY3ki5SqHzr38DbSsGF1DAJjJ-wBpNEAG/file?dl=1# [following]\n",
      "--2023-11-12 19:56:35--  https://uc349d8f9c0125e0b9e0bca55f5e.dl.dropboxusercontent.com/cd/0/get/CHYluIWEANvzu7UTaq26J9HHNDkz7LVj-Idr_Upu5wq4dGyKaPMUI8WR4MIySXwnBEej5oavWoLuzE_zV-dxjqBjFvDHUN9AeJLonGR0DyLm2O7e2s01OzzgY3ki5SqHzr38DbSsGF1DAJjJ-wBpNEAG/file?dl=1\n",
      "Resolving uc349d8f9c0125e0b9e0bca55f5e.dl.dropboxusercontent.com (uc349d8f9c0125e0b9e0bca55f5e.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
      "Connecting to uc349d8f9c0125e0b9e0bca55f5e.dl.dropboxusercontent.com (uc349d8f9c0125e0b9e0bca55f5e.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 220318689 (210M) [application/binary]\n",
      "Saving to: ‘dataset.zip’\n",
      "\n",
      "dataset.zip         100%[===================>] 210.11M   106MB/s    in 2.0s    \n",
      "\n",
      "2023-11-12 19:56:38 (106 MB/s) - ‘dataset.zip’ saved [220318689/220318689]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!pip3 install pytorch_lightning torchmetrics\n",
    "!pip install lion_pytorch\n",
    "!wget https://www.dropbox.com/s/33l8lp62rmvtx40/dataset.zip?dl=1 -O dataset.zip && unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**✨ Внимание ✨**\n",
    "\n",
    "В этом домашнем задании предлагается использовать библиотеку `pytorch_lightning`. Доступ к ее [документации](https://lightning.ai/docs/pytorch/stable/) заблокирован с территории РФ. Вы можете:\n",
    "\n",
    "1. Получить к ней доступ с помощью VPN.\n",
    "\n",
    "2. Собрать документацию самостоятельно. Для этого склонируйте [github-репозиторий](https://github.com/Lightning-AI/lightning/tree/master), запустите в нем терминал (на windows – git bash) и выполните команды:\n",
    "\n",
    "```shell\n",
    "git submodule update --init --recursive\n",
    "make docs\n",
    "```\n",
    "После этого откройте появившийся файл `docs/build/html/index.html`. Для работы команд в вашем окружении должен быть `pip`. Полная инструкция [по ссылке](https://github.com/Lightning-AI/lightning/tree/master/docs).\n",
    "\n",
    "3. Гуглить `<error message> pytorch lightning` или `<how to do this> pytorch lightning`. Stack overflow на территории РФ все еще доступен 😉\n",
    "\n",
    "4. Не пользоваться `pytorch_lightning` и написать цикл обучения модели самостоятельно. Например, по аналогии с функцией `fit` из [семинара 4](https://github.com/hse-ds/iad-deep-learning/blob/master/2023/seminars/04.%20Optim%20%26%20Lightning/04_Optim%26Lightning_solution.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Возможно ниже будет не очень работать WandbLogger с первого разу, перезапустите kernel тетрадки\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from lion_pytorch import Lion\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "\n",
    "# You may add any imports you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 13\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    # Фискирует максимум сидов.\n",
    "    # Это понадобится, чтобы сравнение оптимизаторов было корректным\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.random.manual_seed(seed)\n",
    "    pl.seed_everything(seed=seed)\n",
    "\n",
    "seed_everything(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что поможет сделать на 10 из 10 (одно задание - 5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Использовать все возможные методы оптимизации и эксперемнтировать с ними.\n",
    "2. Подбор learning rate. Пример из прошлого семинара как это делать: [Как найти lr](https://pytorch-lightning.readthedocs.io/en/1.4.5/advanced/lr_finder.html)\n",
    "\n",
    "```\n",
    "  trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=2, auto_lr_find=True)\n",
    "\n",
    "  trainer.tune(module, train_dataloader, eval_dataloader)\n",
    "\n",
    "  trainer.fit(module, train_dataloader, eval_dataloader))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "3. Аугментация данных. [Документация (полезная)](https://pytorch.org/vision/main/transforms.html), а также [библиотека albumentation](https://towardsdatascience.com/getting-started-with-albumentation-winning-deep-learning-image-augmentation-technique-in-pytorch-47aaba0ee3f8)\n",
    "4. Подбор архитектуры модели.\n",
    "5. Можно написать модель руками свою в YourNet, а можно импортировать не предобученную сетку известной архитектуры из модуля torchvision.models. Один из способов как можно сделать:\n",
    "\n",
    "  * `torchvision.models.resnet18(pretrained=False, num_classes=200).to(device)`\n",
    "  * Документация по возможным моделям и как их можно брать: [Документация (полезная)](https://pytorch.org/vision/stable/models.html)\n",
    "6. Правильно нормализовывать данные при создании, пример [тык, но тут и в целом гайд от и до](https://www.pluralsight.com/guides/image-classification-with-pytorch)\n",
    "7. Model Checkpointing. Сохраняйте свой прогресс (модели), чтобы когда что-то пойдет не так вы сможете начать с этого места или просто воспроизвести свои результаты модели, которые обучали.\n",
    " * Пример как можно с wandb тут: [Сохраняем лучшие модели в wandb](https://docs.wandb.ai/guides/integrations/lightning)\n",
    " * По простому можно так: [Сохраняем модели в pytorch дока](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_img = transforms.Compose([\n",
    "    transforms.RandomApply(transforms=[transforms.AutoAugment()], p=0.5),\n",
    "    transforms.RandomApply(transforms=[transforms.AugMix()], p=0.2),\n",
    "    transforms.RandomApply(transforms=[transforms.Grayscale(num_output_channels=3)], p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('/Users/nikitaevseev/Desktop/DS/DL/dataset/dataset/train', transform=transform_img)\n",
    "\n",
    "train_dataset_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=len(train_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean and std: \n",
      " tensor([0.4681, 0.4489, 0.4170]) tensor([0.2760, 0.2705, 0.2814])\n"
     ]
    }
   ],
   "source": [
    "def mean_std(loader):\n",
    "  images, lebels = next(iter(loader))\n",
    "  mean, std = images.mean([0,2,3]), images.std([0,2,3])\n",
    "  return mean, std\n",
    "mean, std = mean_std(train_dataset_loader)\n",
    "print(\"mean and std: \\n\", mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomApply(transforms=[transforms.AutoAugment()], p=0.5),\n",
    "        transforms.RandomApply(transforms=[transforms.AugMix()], p=0.2),\n",
    "        transforms.RandomApply(transforms=[transforms.Grayscale(num_output_channels=3)], p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4681, 0.4489, 0.4170], std=[0.2760, 0.2705, 0.2814])\n",
    "    ]\n",
    ")\n",
    "#(0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)                    [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
    "# mean=[0.4802, 0.4481, 0.3975], std=[0.2770, 0.2691, 0.2821]\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4681, 0.4489, 0.4170], std=[0.2760, 0.2705, 0.2814])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder('/content/dataset/dataset/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('/content/dataset/dataset/val', transform=val_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2, persistent_workers=True, multiprocessing_context='forkserver')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2, persistent_workers=True, multiprocessing_context='forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple sanity checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на картиночки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    images, class_nums = batch\n",
    "    plt.imshow(images[5].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[15].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.\n",
    "\n",
    "5 баллов\n",
    "Добейтесь accuracy на валидации не менее 0.44. В этом задании запрещено пользоваться предобученными моделями и ресайзом картинок.\n",
    "\n",
    "\n",
    "Для того чтобы выбить скор (считается ниже) на 2.5/5 балла (то есть половину за задание) достаточно соблюдать пару простых жизненных правил:\n",
    "1. Аугментация (без нее сложно очень будет)\n",
    "2. Оптимайзеры можно (и нужно) использовать друг с другом. Однако когда что-то проверяете, то не меняйте несколько параметров сразу - собьете логику экспериментов\n",
    "3. Не используйте полносвязные модели или самые первые сверточные, используйте более современные архитектуры (что на лекциях встречались)\n",
    "4. Посмотреть все ноутбуки прошедших семинаров и слепить из них что-то общее. Семинарских тетрадок хватит сверх"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель (или просто импортируйте не предобученную)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourNet(torch.nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.batch_norm1 = torch.nn.BatchNorm2d(16)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.batch_norm3 = torch.nn.BatchNorm2d(64)\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(128)\n",
    "\n",
    "        self.maxpool = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 16, 3)\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, 3)\n",
    "        self.conv4 = torch.nn.Conv2d(32, 32, 3)\n",
    "        self.conv5 = torch.nn.Conv2d(32, 64, 3)\n",
    "        self.conv6 = torch.nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(64 * 4 * 4, 128)\n",
    "        self.output = torch.nn.Linear(128, 200)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # --------------\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.targets = torch.Tensor()\n",
    "        self.preds = torch.Tensor()\n",
    "        pass\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def forward(self, images, target=None):\n",
    "        # images ~ (batch size, num channels, height, width)\n",
    "        # target ~ (batch size)\n",
    "        # output ~ (batch size, num classes)\n",
    "        output = self._forward(images)\n",
    "\n",
    "        # get accuracy score and save it to self.accuracy\n",
    "        if target is not None:\n",
    "            loss = self.loss_func(output, target)\n",
    "\n",
    "            self.targets = torch.cat((self.targets, target.cpu()), 0)\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            self.preds = torch.cat((self.preds, pred.cpu()), 0)\n",
    "            self.accuracy = accuracy(self.preds.long(), self.targets.long(), task='multiclass', num_classes=200)\n",
    "\n",
    "        return loss if target is not None else output\n",
    "\n",
    "    def get_accuracy(self, reset=False):\n",
    "        # return accuracy by all values till now\n",
    "        if reset:\n",
    "            self.targets = torch.Tensor()\n",
    "            self.preds = torch.Tensor()\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти попытки не вошли в итоговый зачет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировочный класс lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4, wd=10.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wd = wd\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.model(x)\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Lion(self.parameters(), lr=self.learning_rate, weight_decay=self.wd)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(optimizer, factor=0.1, patience=1),\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "                },\n",
    "        }\n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        images, target = train_batch\n",
    "        output = self.model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, prog_bar=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        images, target = val_batch\n",
    "        output = self.model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        pred = torch.argmax(output, dim=-1)\n",
    "        acc_batch = accuracy(pred.long(), target, task='multiclass', num_classes=200)\n",
    "        self.log(\"acc\", acc_batch, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False, num_classes=200)\n",
    "\n",
    "checkpoint = torch.load('/Users/nikitaevseev/Desktop/DS/DL/lightning_logs/ye436ozw/checkpoints/epoch=11-step=2352.ckpt')\n",
    "\n",
    "prefix_to_remove = 'model.'\n",
    "new_state_dict = {k.replace(prefix_to_remove, ''): v for k, v in checkpoint['state_dict'].items()}\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=False, num_classes=200)\n",
    "\n",
    "wandb_logger = WandbLogger\\\n",
    "    (log_model='all')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = resnet\n",
    "module = YourModule(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t8wrzoey) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf706ebf488b46d3b19a58e44046cd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final</strong> at: <a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/t8wrzoey' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1/runs/t8wrzoey</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_195752-t8wrzoey/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t8wrzoey). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20231112_195820-2kowz8zs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/2kowz8zs' target=\"_blank\">final</a></strong> to <a href='https://wandb.ai/naevseev-work/hw_02_exs1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naevseev-work/hw_02_exs1' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/2kowz8zs' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1/runs/2kowz8zs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"final\"\n",
    "wandb.init(project=\"hw_02_exs1\", name=name)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.3 M\n",
      "---------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.116    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e2e74ab2664d96afdfa65b057f1c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b01b66dfe34ea68c676ac5cb087b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae24aaced894e2f9bb7fdf82fa863fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3aee47a245d4252b7c468a126578252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cc47cfad8042d6ab3e660cd8ad832d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b0091a1cf74de0a8ef0e2689156729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf158b8d65d4f8880133b7cf1da3bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2eb6a58b7284deb9e1b7ce05a7de1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cef3aa1a2b942fb9093b77d00ba93aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a116c12f1a4a7d955c83a2852daad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc92aabe3564b8d9702e0edff05f67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a016a618690140eeaa4a59bd7dccc40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe74db6da3a445d8cfb089ade8e3c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403ad4e11d9048c38b4be0a951142fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebd6bc98dae4ccd800a4d6119338658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566e5eec9155444592116911fa5f24a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d136bf4c48904094b285ddb26d9f3539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006c2364fce34e8ca57a737fa7cd2389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8860868ebb4c8ca6216e98ef49aae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a696b22aad684ada96576320c674dccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cc5255c61d4e33a4c9c813c2947fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb24205ecb224b978a9df327401ad430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"auto\", logger=wandb_logger, max_epochs=20)\n",
    "trainer.fit(module, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Валидация результатов задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(model, test_dataloader):\n",
    "    model = model\n",
    "    model.eval()\n",
    "    taccuracy = 0.0\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images, labels = images, labels\n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            acc_batch = accuracy(pred.long(), labels.long(), task='multiclass', num_classes=200)\n",
    "        taccuracy += acc_batch\n",
    "    taccuracy = taccuracy / len(test_dataloader)\n",
    "    return taccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:37<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка за это задание составит 10.00 баллов\n",
      "tensor(0.4466)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = evaluate_task(model, val_dataloader)\n",
    "print(f\"Оценка за это задание составит {np.clip(10 * accuracy_score / 0.44, 0, 10):.2f} баллов\")\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более глубокие модели ResNet50-200 слишком долго обучаются. То же можно сказать про меньший batch_size, например 64, 128, а также иные оптимизаторы (AdamW).\n",
    "InseptonV3, ResNext50_32x4d, ConvNext_tiny - все они либо не работают с маленькими картинками, либо слишком тяжелые для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/naevseev-work/hw_02_exs1/reports/hw_dl_02_exs1--Vmlldzo1OTQ3ODM5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "5 баллов\n",
    "Добейтесь accuracy на валидации не менее 0.84. В этом задании делать ресайз и использовать претрейн можно.\n",
    "\n",
    "Для того чтобы выбить скор (считается ниже) на 2.5/5 балла (то есть половину за задание) достаточно соблюдать пару простых жизненных правил:\n",
    "1. Аугментация (без нее сложно очень будет)\n",
    "2. Оптимайзеры можно (и нужно) использовать друг с другом. Однако когда что-то проверяете, то не меняйте несколько параметров сразу - собьете логику экспериментов\n",
    "3. Не используйте полносвязные модели или самые первые сверточные, используйте более современные архитектуры (что на лекциях встречались или можете пойти дальше).\n",
    "4. Попробуйте сначала посмотреть качество исходной модели без дообучения, сохраните как baseline. Отсюда поймете какие слои нужно дообучать.\n",
    "5. Посмотреть все ноутбуки прошедших семинаров и слепить из них что-то общее. Семинарских тетрадок хватит сверх"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.disable_beta_transforms_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        v2.Resize(144),\n",
    "        v2.RandomApply(transforms=[v2.AutoAugment()], p=0.3),\n",
    "        v2.RandomApply(transforms=[v2.AugMix()], p=0.3),\n",
    "        #v2.RandomApply(transforms=[v2.Grayscale(num_output_channels=3)], p=0.1),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean=[0.4687, 0.4491, 0.4163], std=[0.2716, 0.2662, 0.2772])\n",
    "    ]\n",
    ")\n",
    "#(0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)                    [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
    "# mean=[0.4802, 0.4481, 0.3975], std=[0.2770, 0.2691, 0.2821]\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        v2.Resize(144),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean=[0.4687, 0.4491, 0.4163], std=[0.2716, 0.2662, 0.2772])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder('/content/dataset/dataset/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('/content/dataset/dataset/val', transform=val_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2, persistent_workers=True, multiprocessing_context='forkserver')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2, persistent_workers=True, multiprocessing_context='forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple sanity checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    images, class_nums = batch\n",
    "    plt.imshow(images[5].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[15].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировочный класс lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourModule(pl.LightningModule):\n",
    "    def __init__(self, model=None, classifier=None, learning_rate=1e-4, wd=10.0) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wd = wd\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.model(x)\n",
    "        preds = self.classifier(features)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Lion(self.classifier.parameters(), lr=self.learning_rate, weight_decay=self.wd)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(optimizer, factor=0.1, patience=1),\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "                },\n",
    "        }\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        images, target = train_batch\n",
    "        output = self.forward(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, prog_bar=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        images, target = val_batch\n",
    "        output = self.forward(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        pred = torch.argmax(output, dim=-1)\n",
    "        acc_batch = accuracy(pred.long(), target, task='multiclass', num_classes=200)\n",
    "        self.log(\"acc\", acc_batch, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Large_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.convnext_large(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convnext_large\n",
    "classifier = nn.Linear(in_features=model.classifier[2].in_features, out_features=200, bias=True)\n",
    "model.classifier[2] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convnext_base\n",
    "classifier = nn.Linear(in_features=model.classifier[2].in_features, out_features=200, bias=True)\n",
    "model.classifier[2] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_b_16\n",
    "classifier = nn.Linear(in_features=model.heads.head.in_features, out_features=200, bias=True)\n",
    "model.heads.head = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swin_v2_b\n",
    "classifier = nn.Linear(in_features=model.head.in_features, out_features=200, bias=True)\n",
    "model.head = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet18\n",
    "classifier = nn.Linear(in_features=model.fc.in_features, out_features=200, bias=True)\n",
    "model.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20231112_174020-n8o5hocq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naevseev-work/hw_02_exs2/runs/n8o5hocq' target=\"_blank\">convnext_large+Lion+256+144^2+wd=10+aug+finetune</a></strong> to <a href='https://wandb.ai/naevseev-work/hw_02_exs2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naevseev-work/hw_02_exs2' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naevseev-work/hw_02_exs2/runs/n8o5hocq' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs2/runs/n8o5hocq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type     | Params\n",
      "----------------------------------------\n",
      "0 | model      | ConvNeXt | 196 M \n",
      "1 | classifier | Linear   | 307 K \n",
      "----------------------------------------\n",
      "196 M     Trainable params\n",
      "0         Non-trainable params\n",
      "196 M     Total params\n",
      "786.151   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c261fb02e5e34592a5ef21c34c25c66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebed924ea6d7496dacf6b10e396ec68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccdb59f08e44b5ca27e754e9a90c7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaa249feaf54c8397b516fab12f84ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3369aeee6174b7d9c3157f1b9ea3480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63db40938bbf473280e740d65c9d6b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger\\\n",
    "    (log_model='all')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "module = YourModule(model, classifier)\n",
    "\n",
    "name = \"convnext_large+Lion+256+144^2+wd=10+aug+finetune\"\n",
    "wandb.init(project=\"hw_02_exs2\", name=name)\n",
    "wandb.watch(model)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"auto\", logger=wandb_logger, max_epochs=4)\n",
    "trainer.fit(module, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Валидация результатов задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(module, test_dataloader):\n",
    "    module = module.to(device)\n",
    "    module.eval()\n",
    "    taccuracy = 0.0\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = module.forward(images)\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            acc_batch = accuracy(pred.long(), labels.long(), task='multiclass', num_classes=200)\n",
    "        taccuracy += acc_batch\n",
    "    taccuracy = taccuracy / len(test_dataloader)\n",
    "    return taccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:33<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка за это задание составит 10.00 баллов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aaccuracy = evaluate_task(module, val_dataloader).to(\"cpu\")\n",
    "print(f\"Оценка за это задание составит {np.clip(10 * (aaccuracy - 0.5) / 0.34, 0, 10):.2f} баллов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отчёт об экспериментах\n",
    "\n",
    "текст писать тут (или ссылочку на wandb/любой трекер экспреиментов) для каждого задания, то есть не обязательно именно тут рисовать графики, если вы используете готовые трекеры/мониторинги ваших моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.wandb.ai/links/naevseev-work/rqr83yci - ссылка на отчет по 2 заданию, там же описание экспериментов"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "max_cell_id": 35
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
