{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 1,
    "id": "kr9vAeEQlRVG"
   },
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 3,
    "id": "BxX49gLclRVJ"
   },
   "source": [
    "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ë—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º, –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å –Ω–µ –±—É–¥–µ–º. –ú–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤ –µ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–µ. –í –Ω—ë–º 200 –∫–ª–∞—Å—Å–æ–≤ –∏ –æ–∫–æ–ª–æ 5 —Ç—ã—Å—è—á –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–∞ –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Å. –ö–ª–∞—Å—Å—ã –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω—ã, –∫–∞–∫ –Ω–µ—Ç—Ä—É–¥–Ω–æ –¥–æ–≥–∞–¥–∞—Ç—å—Å—è, –æ—Ç 0 –¥–æ 199. –°–∫–∞—á–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–Ω–æ –≤–æ—Ç [—Ç—É—Ç](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–æ—Å—Ç–∞—è -- –µ—Å—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ train/ –∏ val/, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ª–µ–∂–∞—Ç –æ–±—É—á–∞—é—â–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –í train/ –∏ val/ –ª–µ–∂–∞—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ª–µ–∂–∞—Ç, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ, —Å–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    " \n",
    "__–ó–∞–¥–∞–Ω–∏–µ__. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –¥–≤–∞ –∑–∞–¥–∞–Ω–∏—è\n",
    "\n",
    "1) –î–æ–±–µ–π—Ç–µ—Å—å accuracy **–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.44**. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ **–∑–∞–ø—Ä–µ—â–µ–Ω–æ** –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ä–µ—Å–∞–π–∑–æ–º –∫–∞—Ä—Ç–∏–Ω–æ–∫. 5 –±–∞–ª–ª–æ–≤\n",
    "\n",
    "2) –î–æ–±–µ–π—Ç–µ—Å—å accuracy **–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.84**. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –¥–µ–ª–∞—Ç—å —Ä–µ—Å–∞–π–∑ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ—Ç—Ä–µ–π–Ω –º–æ–∂–Ω–æ. 5 –±–∞–ª–ª–æ–≤\n",
    "\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö. –ß—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –∏ —á—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ? –ü–æ—á–µ–º—É –≤—ã —Ä–µ—à–∏–ª–∏, —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, –∞ –Ω–µ –∏–Ω–∞—á–µ? –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —á—É–∂–æ–π –∫–æ–¥, –µ—Å–ª–∏ –≤—ã –µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å—Å—ã–ª–∞–π—Ç–µ—Å—å –Ω–∞ —Å—Ç–∞—Ç—å–∏ / –±–ª–æ–≥–ø–æ—Å—Ç—ã / –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ stackoverflow / –≤–∏–¥–æ—Å—ã –æ—Ç —é—Ç—É–±–µ—Ä–æ–≤-–º–∞—à–∏–Ω–ª–µ—Ä–Ω–µ—Ä–æ–≤ / –∫—É—Ä—Å—ã / –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç –î—è–¥–∏ –í–∞—Å–∏ –∏ –ø—Ä–æ—á–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –µ—Å–ª–∏ –≤—ã –∏—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ. \n",
    "\n",
    "–í–∞—à –∫–æ–¥ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–µ–Ω –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –≤—Å–µ `assert`'—ã –Ω–∏–∂–µ.\n",
    "\n",
    "__–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–æ–≥–æ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –≤ –æ–±–æ–∏—Ö –∑–∞–¥–∞–Ω–∏—è—Ö. –¢–∞–∫–∂–µ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ__.\n",
    "\n",
    "\n",
    "__–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏__: –û—Ü–µ–Ω–∫–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–æ –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º—É–ª–µ: `min(10, 10 * –í–∞—à–∞ accuracy / 0.44)` –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –∏ `min(10, 10 * (–í–∞—à–∞ accuracy - 0.5) / 0.34)` –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ. –û—Ü–µ–Ω–∫–∞ –æ–∫—Ä—É–≥–ª—è–µ—Ç—Å—è –¥–æ –¥–µ—Å—è—Ç—ã—Ö –ø–æ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∞–≤–∏–ª–∞–º.\n",
    "\n",
    "\n",
    "__–°–æ–≤–µ—Ç—ã –∏ —É–∫–∞–∑–∞–Ω–∏—è__:\n",
    " - –ù–∞–≤–µ—Ä–Ω—è–∫–∞ –≤–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–Ω–æ–≥–æ –≥—É–≥–ª–∏—Ç—å –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –æ —Ç–æ–º, –∫–∞–∫ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –µ—ë —Ä–∞–±–æ—Ç–∞—Ç—å. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –≤—Å–µ –≥—É–≥–ª—è—Ç. –ù–æ –Ω–µ –∑–∞–±—ã–≤–∞–π—Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –±—ã—Ç—å –≥–æ—Ç–æ–≤—ã–º –∑–∞ —Å–∫–∞—Ç–∞–Ω–Ω—ã–π –∫–æ–¥ –æ—Ç–≤–µ—á–∞—Ç—å :)\n",
    " - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –î–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –º–æ–¥—É–ª–µ–º `torchvision.transforms` –∏–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - –ú–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å —Å –Ω—É–ª—è –∏–ª–∏ —Ñ–∞–π–Ω—Ç—é–Ω–∏—Ç—å (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞–Ω–∏—è) –º–æ–¥–µ–ª–∏ –∏–∑ `torchvision`.\n",
    " - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å –≤–∞–º —Å–Ω–∞—á–∞–ª–∞ –∫–ª–∞—Å—Å-–¥–∞—Ç–∞—Å–µ—Ç (–∏–ª–∏ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–ª–∞—Å—Å–æ–º `ImageFolder`), –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–º –∫–ª–∞—Å—Å—ã, –∞ –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –ø–æ —à–∞–±–ª–æ–Ω–∞–º –Ω–∏–∂–µ. –û–¥–Ω–∞–∫–æ –¥–µ–ª–∞—Ç—å —ç—Ç–æ –º—ã –Ω–µ –∑–∞—Å—Ç–∞–≤–ª—è–µ–º. –ï—Å–ª–∏ –≤–∞–º —Ç–∞–∫ –Ω–µ—É–¥–æ–±–Ω–æ, —Ç–æ –º–æ–∂–µ—Ç–µ –ø–∏—Å–∞—Ç—å –∫–æ–¥ –≤ —É–¥–æ–±–Ω–æ–º —Å—Ç–∏–ª–µ. –û–¥–Ω–∞–∫–æ —É—á—Ç–∏—Ç–µ, —á—Ç–æ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∏–∂–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ —É–≤–µ–ª–∏—á–∏—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –∫ –≤–∞—à–µ–º—É –∫–æ–¥—É –∏ –ø–æ–≤—ã—Å–∏—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–∑–æ–≤–∞ –Ω–∞ –∑–∞—â–∏—Ç—É :)\n",
    " - –í–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ. –¢—Ä–µ–∫–∞–π—Ç–µ –æ—à–∏–±–∫–∏ –∫–∞–∫ –º–æ–∂–Ω–æ —Ä–∞–Ω—å—à–µ, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –≤–ø—É—Å—Ç—É—é.\n",
    " - –ß—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –æ—Ç–ª–∞–¥–∏—Ç—å –∫–æ–¥, –ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–π —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Å–∫–∞–∂–µ–º, 5-10 –∫–∞—Ä—Ç–∏–Ω–æ–∫ –ø—Ä–æ—Å—Ç–æ —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –∫–æ–¥ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è). –ö–æ–≥–¥–∞ –≤—ã –ø–æ–Ω—è–ª–∏, —á—Ç–æ —Å–º–æ–≥–ª–∏ –≤—Å—ë –æ—Ç–¥–µ–±–∞–∂–∏—Ç—å, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –æ–±—É—á–µ–Ω–∏—é –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    " - –ù–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—É—Å–∫ –¥–µ–ª–∞–π—Ç–µ —Ä–æ–≤–Ω–æ –æ–¥–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ –º–æ–¥–µ–ª–∏/–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏/–æ–ø—Ç–∏–º–∞–π–∑–µ—Ä–µ, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –∏ –∫–∞–∫ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n",
    " - –§–∏–∫—Å–∏—Ä—É–π—Ç–µ random seed.\n",
    " - –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–æ–∂–Ω—ã–º. –û–±—É—á–µ–Ω–∏–µ –ª—ë–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —ç–∫–æ–Ω–æ–º–∏—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.\n",
    " - –°—Ç–∞–≤—å—Ç–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ learning rate. –£–º–µ–Ω—å—à–∞–π—Ç–µ –µ–≥–æ, –∫–æ–≥–¥–∞ –ª–æ—Å—Å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç —É–±—ã–≤–∞—Ç—å.\n",
    " - –°–æ–≤–µ—Ç—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU. –ï—Å–ª–∏ —É –≤–∞—Å –µ–≥–æ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ google colab. –ï—Å–ª–∏ –≤–∞–º –Ω–µ—É–¥–æ–±–Ω–æ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π –æ—Å–Ω–æ–≤–µ, –Ω–∞–ø–∏—à–∏—Ç–µ –∏ –æ—Ç–ª–∞–¥—å—Ç–µ –≤–µ—Å—å –∫–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ CPU, –∞ –∑–∞—Ç–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç–µ —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–π –Ω–æ—É—Ç–±—É–∫ –≤ –∫–æ–ª–∞–±–µ. –ê–≤—Ç–æ—Ä—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç—Ä–µ–±—É–µ–º–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∫–æ–ª–∞–±–µ –∑–∞ 15 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BaPSFxIz96tY"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "# !pip3 install pytorch_lightning torchmetrics  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ú® –í–Ω–∏–º–∞–Ω–∏–µ ‚ú®**\n",
    "\n",
    "–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É `pytorch_lightning`. –î–æ—Å—Ç—É–ø –∫ –µ–µ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏](https://lightning.ai/docs/pytorch/stable/) –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–§. –í—ã –º–æ–∂–µ—Ç–µ:\n",
    "\n",
    "1. –ü–æ–ª—É—á–∏—Ç—å –∫ –Ω–µ–π –¥–æ—Å—Ç—É–ø —Å –ø–æ–º–æ—â—å—é VPN.\n",
    "\n",
    "2. –°–æ–±—Ä–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –î–ª—è —ç—Ç–æ–≥–æ —Å–∫–ª–æ–Ω–∏—Ä—É–π—Ç–µ [github-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π](https://github.com/Lightning-AI/lightning/tree/master), –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤ –Ω–µ–º —Ç–µ—Ä–º–∏–Ω–∞–ª (–Ω–∞ windows ‚Äì git bash) –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—ã:\n",
    "\n",
    "```shell\n",
    "git submodule update --init --recursive\n",
    "make docs\n",
    "```\n",
    "–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ—Ç–∫—Ä–æ–π—Ç–µ –ø–æ—è–≤–∏–≤—à–∏–π—Å—è —Ñ–∞–π–ª `docs/build/html/index.html`. –î–ª—è —Ä–∞–±–æ—Ç—ã –∫–æ–º–∞–Ω–¥ –≤ –≤–∞—à–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å `pip`. –ü–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è [–ø–æ —Å—Å—ã–ª–∫–µ](https://github.com/Lightning-AI/lightning/tree/master/docs).\n",
    "\n",
    "3. –ì—É–≥–ª–∏—Ç—å `<error message> pytorch lightning` –∏–ª–∏ `<how to do this> pytorch lightning`. Stack overflow –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–§ –≤—Å–µ –µ—â–µ –¥–æ—Å—Ç—É–ø–µ–Ω üòâ\n",
    "\n",
    "4. –ù–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è `pytorch_lightning` –∏ –Ω–∞–ø–∏—Å–∞—Ç—å —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π `fit` –∏–∑ [—Å–µ–º–∏–Ω–∞—Ä–∞ 4](https://github.com/hse-ds/iad-deep-learning/blob/master/2023/seminars/04.%20Optim%20%26%20Lightning/04_Optim%26Lightning_solution.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EWT3aFU9XmLJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaevseev-work\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –í–æ–∑–º–æ–∂–Ω–æ –Ω–∏–∂–µ –±—É–¥–µ—Ç –Ω–µ –æ—á–µ–Ω—å —Ä–∞–±–æ—Ç–∞—Ç—å WandbLogger —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑—É, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ kernel —Ç–µ—Ç—Ä–∞–¥–∫–∏\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": 4,
    "id": "LKcSNj4tlRVK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You may add any imports you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "47YPLjDL-Mtv"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    # –§–∏—Å–∫–∏—Ä—É–µ—Ç –º–∞–∫—Å–∏–º—É–º —Å–∏–¥–æ–≤.\n",
    "    # –≠—Ç–æ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è, —á—Ç–æ–±—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –±—ã–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RytEDW0ylRVN"
   },
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HZECedTvepi"
   },
   "source": [
    "### –ß—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å –Ω–∞ 10 –∏–∑ 10 (–æ–¥–Ω–æ –∑–∞–¥–∞–Ω–∏–µ - 5 –±–∞–ª–ª–æ–≤)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOioHGEiveso"
   },
   "source": [
    "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —ç–∫—Å–ø–µ—Ä–µ–º–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –Ω–∏–º–∏.\n",
    "2. –ü–æ–¥–±–æ—Ä learning rate. –ü—Ä–∏–º–µ—Ä –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ —Å–µ–º–∏–Ω–∞—Ä–∞ –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞—Ç—å: [–ö–∞–∫ –Ω–∞–π—Ç–∏ lr](https://pytorch-lightning.readthedocs.io/en/1.4.5/advanced/lr_finder.html)\n",
    "\n",
    "```\n",
    "  trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=2, auto_lr_find=True) \n",
    "\n",
    "  trainer.tune(module, train_dataloader, eval_dataloader)\n",
    "\n",
    "  trainer.fit(module, train_dataloader, eval_dataloader))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "3. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö. [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–ø–æ–ª–µ–∑–Ω–∞—è)](https://pytorch.org/vision/main/transforms.html), –∞ —Ç–∞–∫–∂–µ [–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ albumentation](https://towardsdatascience.com/getting-started-with-albumentation-winning-deep-learning-image-augmentation-technique-in-pytorch-47aaba0ee3f8)\n",
    "4. –ü–æ–¥–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏. \n",
    "5. –ú–æ–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –º–æ–¥–µ–ª—å —Ä—É–∫–∞–º–∏ —Å–≤–æ—é –≤ YourNet, –∞ –º–æ–∂–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é —Å–µ—Ç–∫—É –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–∑ –º–æ–¥—É–ª—è torchvision.models. –û–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –∫–∞–∫ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å: \n",
    "\n",
    "  * `torchvision.models.resnet18(pretrained=False, num_classes=200).to(device)`\n",
    "  * –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –≤–æ–∑–º–æ–∂–Ω—ã–º –º–æ–¥–µ–ª—è–º –∏ –∫–∞–∫ –∏—Ö –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å: [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–ø–æ–ª–µ–∑–Ω–∞—è)](https://pytorch.org/vision/stable/models.html)\n",
    "6. –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏, –ø—Ä–∏–º–µ—Ä [—Ç—ã–∫, –Ω–æ —Ç—É—Ç –∏ –≤ —Ü–µ–ª–æ–º –≥–∞–π–¥ –æ—Ç –∏ –¥–æ](https://www.pluralsight.com/guides/image-classification-with-pytorch)\n",
    "7. Model Checkpointing. –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Å–≤–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å (–º–æ–¥–µ–ª–∏), —á—Ç–æ–±—ã –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥–µ—Ç –Ω–µ —Ç–∞–∫ –≤—ã —Å–º–æ–∂–µ—Ç–µ –Ω–∞—á–∞—Ç—å —Å —ç—Ç–æ–≥–æ –º–µ—Å—Ç–∞ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞–ª–∏. \n",
    " * –ü—Ä–∏–º–µ—Ä –∫–∞–∫ –º–æ–∂–Ω–æ —Å wandb —Ç—É—Ç: [–°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –≤ wandb](https://docs.wandb.ai/guides/integrations/lightning)\n",
    " * –ü–æ –ø—Ä–æ—Å—Ç–æ–º—É –º–æ–∂–Ω–æ —Ç–∞–∫: [–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ –≤ pytorch –¥–æ–∫–∞](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYePsQgNRB-n"
   },
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_img = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('/Users/nikitaevseev/Desktop/DS/DL/dataset/dataset/train', transform=transform_img)\n",
    "\n",
    "train_dataset_loader = DataLoader(\n",
    "  train_dataset, \n",
    "  batch_size=len(train_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean and std: \n",
      " tensor([0.4802, 0.4481, 0.3975]) tensor([0.2770, 0.2691, 0.2821])\n"
     ]
    }
   ],
   "source": [
    "def mean_std(loader):\n",
    "  images, lebels = next(iter(loader))\n",
    "  mean, std = images.mean([0,2,3]), images.std([0,2,3])\n",
    "  return mean, std\n",
    "mean, std = mean_std(train_dataset_loader)\n",
    "print(\"mean and std: \\n\", mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": 5,
    "id": "QEdDQtHdlRVO"
   },
   "outputs": [],
   "source": [
    "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomApply(transforms=[transforms.ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1)], p=0.2),\n",
    "        transforms.RandomApply(transforms=[transforms.AugMix()], p=0.25),\n",
    "        transforms.RandomApply(transforms=[transforms.Grayscale(num_output_channels=3)], p=0.07),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2770, 0.2691, 0.2821])\n",
    "    ]\n",
    ")\n",
    "#(0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)                    [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2770, 0.2691, 0.2821])\n",
    "    ]\n",
    ")\n",
    "\n",
    "#applier = transforms.RandomApply(transforms=[AugMix], p=0.5)\n",
    "\n",
    "#transforms = transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(), ]), p=0.3)\n",
    "\n",
    "train_dataset = ImageFolder('/Users/nikitaevseev/Desktop/DS/DL/dataset/dataset/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('/Users/nikitaevseev/Desktop/DS/DL/dataset/dataset/val', transform=val_transform)\n",
    "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=7, persistent_workers=True, multiprocessing_context='forkserver')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=7, persistent_workers=True, multiprocessing_context='forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": 6,
    "id": "mrg4Yj0VlRVP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple sanity checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOuM0EEYj7Ml"
   },
   "source": [
    "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–æ—á–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeuB0YC3LYRm"
   },
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    images, class_nums = batch\n",
    "    plt.imshow(images[5].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(images[15].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCwKB-3nKm1-"
   },
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 1. \n",
    "\n",
    "5 –±–∞–ª–ª–æ–≤\n",
    "–î–æ–±–µ–π—Ç–µ—Å—å accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.44. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ä–µ—Å–∞–π–∑–æ–º –∫–∞—Ä—Ç–∏–Ω–æ–∫.\n",
    "\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤—ã–±–∏—Ç—å —Å–∫–æ—Ä (—Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–∏–∂–µ) –Ω–∞ 2.5/5 –±–∞–ª–ª–∞ (—Ç–æ –µ—Å—Ç—å –ø–æ–ª–æ–≤–∏–Ω—É –∑–∞ –∑–∞–¥–∞–Ω–∏–µ) –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø–∞—Ä—É –ø—Ä–æ—Å—Ç—ã—Ö –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª:\n",
    "1. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–±–µ–∑ –Ω–µ–µ —Å–ª–æ–∂–Ω–æ –æ—á–µ–Ω—å –±—É–¥–µ—Ç)\n",
    "2. –û–ø—Ç–∏–º–∞–π–∑–µ—Ä—ã –º–æ–∂–Ω–æ (–∏ –Ω—É–∂–Ω–æ) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –û–¥–Ω–∞–∫–æ –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç–µ, —Ç–æ –Ω–µ –º–µ–Ω—è–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ä–∞–∑—É - —Å–æ–±—å–µ—Ç–µ –ª–æ–≥–∏–∫—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "3. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Å–∞–º—ã–µ –ø–µ—Ä–≤—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (—á—Ç–æ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ö –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å)\n",
    "4. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ –Ω–æ—É—Ç–±—É–∫–∏ –ø—Ä–æ—à–µ–¥—à–∏—Ö —Å–µ–º–∏–Ω–∞—Ä–æ–≤ –∏ —Å–ª–µ–ø–∏—Ç—å –∏–∑ –Ω–∏—Ö —á—Ç–æ-—Ç–æ –æ–±—â–µ–µ. –°–µ–º–∏–Ω–∞—Ä—Å–∫–∏—Ö —Ç–µ—Ç—Ä–∞–¥–æ–∫ —Ö–≤–∞—Ç–∏—Ç —Å–≤–µ—Ä—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWR2l6ymZfRJ"
   },
   "source": [
    "### –ú–æ–¥–µ–ª—å (–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –Ω–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mDYorQXLZhTQ"
   },
   "outputs": [],
   "source": [
    "class YourNet(torch.nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.batch_norm1 = torch.nn.BatchNorm2d(16)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.batch_norm3 = torch.nn.BatchNorm2d(64)\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(128)\n",
    "\n",
    "        self.maxpool = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 16, 3)\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, 3)\n",
    "        self.conv4 = torch.nn.Conv2d(32, 32, 3)\n",
    "        self.conv5 = torch.nn.Conv2d(32, 64, 3)\n",
    "        self.conv6 = torch.nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(64 * 4 * 4, 128)\n",
    "        self.output = torch.nn.Linear(128, 200)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # --------------\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.targets = torch.Tensor()\n",
    "        self.preds = torch.Tensor()\n",
    "        pass\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def forward(self, images, target=None):\n",
    "        # images ~ (batch size, num channels, height, width)\n",
    "        # target ~ (batch size)\n",
    "        # output ~ (batch size, num classes)\n",
    "        output = self._forward(images)\n",
    "\n",
    "        # get accuracy score and save it to self.accuracy\n",
    "        if target is not None:\n",
    "            loss = self.loss_func(output, target)\n",
    "\n",
    "            self.targets = torch.cat((self.targets, target.cpu()), 0)\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            self.preds = torch.cat((self.preds, pred.cpu()), 0)\n",
    "            self.accuracy = accuracy(self.preds.long(), self.targets.long(), task='multiclass', num_classes=200)\n",
    "\n",
    "        return loss if target is not None else output\n",
    "\n",
    "    def get_accuracy(self, reset=False):\n",
    "        # return accuracy by all values till now\n",
    "        if reset:\n",
    "            self.targets = torch.Tensor()\n",
    "            self.preds = torch.Tensor()\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel\n",
    "\n",
    "#processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "#model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "config = ViTModel.from_pretrained(model_name).config\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å–≤–æ–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "num_classes = 200\n",
    "image_size = 64\n",
    "\n",
    "# –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –≤–∞—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "config.vocab_size = num_classes\n",
    "config.image_size = (image_size, image_size)\n",
    "\n",
    "# –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "model = ViTForImageClassification(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å vit-g/14\n",
    "\n",
    "model = transformers.ViTForImageClassification.from_pretrained(\"google/vit-g-14\", pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7iHXWj1alM1"
   },
   "source": [
    "### –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∫–ª–∞—Å—Å lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lion_pytorch import Lion\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lD76TeZ1apua"
   },
   "outputs": [],
   "source": [
    "class YourModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4, wd=10.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wd = wd\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.model(x)\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate*10)\n",
    "        optimizer = Lion(self.parameters(), lr=self.learning_rate, weight_decay=self.wd)\n",
    "        return {\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": {\n",
    "            \"scheduler\": ReduceLROnPlateau(optimizer, factor=0.1),\n",
    "            \"monitor\": \"train_loss\",\n",
    "            \"frequency\": 1\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        images, target = train_batch\n",
    "        output = self.model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, prog_bar=True\n",
    "        )  # —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥–∏ –≤ –ø–∞–ø–∫—É, –Ω–æ –º–æ–∂–Ω–æ –Ω–µ—Å–ª–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å wandb\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        images, target = val_batch\n",
    "        #loss = self.model(images, target)\n",
    "        output = self.model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(model)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=2, auto_lr_find=True) \n",
    "\n",
    "trainer.tune(module, train_dataloader, val_dataloader)\n",
    "\n",
    "trainer.fit(module, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaevseev/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikitaevseev/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#resnet = torchvision.models.shufflenet_v2_x1_5(pretrained=False, num_classes=200)\n",
    "resnet = torchvision.models.resnet18(pretrained=False, num_classes=200)\n",
    "#num_classes = 200  # –ó–∞–¥–∞–π—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏\n",
    "#resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "z9-1wq7QYkiz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rejvqnvp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c496a782abd490381325736a0619936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">shufflenet_x1.5+Lion+entr+512</strong> at: <a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/rejvqnvp' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1/runs/rejvqnvp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231103_160141-rejvqnvp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rejvqnvp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nikitaevseev/Desktop/DS/DL/wandb/run-20231103_161049-njxtpc4x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/njxtpc4x' target=\"_blank\">resnet18+Lion+entr+512v3</a></strong> to <a href='https://wandb.ai/naevseev-work/hw_02_exs1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naevseev-work/hw_02_exs1' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/njxtpc4x' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1/runs/njxtpc4x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger\\\n",
    "    (log_model='all') # –∫–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–∞–µ—Ç —Å pytorch_lightning https://docs.wandb.ai/guides/integrations/lightning\n",
    "\n",
    "model = resnet#YourNet(dropout=0.4)\n",
    "module = YourModule(model) # YOUR CODE HERE\n",
    "\n",
    "# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç\n",
    "wandb.init(project=\"hw_02_exs1\", name=\"resnet18+Lion+entr+512v3\")\n",
    "# —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∫–∏ –≤ wandb + –ø—Ä–æ—Å–∏–º —Å–ª–µ–¥–∏—Ç—å –∑–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ —Å–µ—Ç–∫–∏\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "If8fi4HZkN3J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/nikitaevseev/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.3 M\n",
      "---------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.116    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcf51d981494a00b2b8a6434327e0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.63 GB, other allocations: 2.38 GB, max allowed: 9.07 GB). Tried to allocate 128.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb –Ø—á–µ–π–∫–∞ 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(logger\u001b[39m=\u001b[39mwandb_logger, max_epochs\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(module, train_dataloader, val_dataloader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m call\u001b[39m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    546\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    547\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(model, ckpt_path\u001b[39m=\u001b[39mckpt_path)\n\u001b[1;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1034\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1035\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1036\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m val_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m   1065\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb –Ø—á–µ–π–∫–∞ 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb#X40sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m images, target \u001b[39m=\u001b[39m val_batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb#X40sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m#loss = self.model(images, target)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb#X40sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb#X40sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikitaevseev/Desktop/DS/DL/DL_hw_02.ipynb#X40sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, loss, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/resnet.py:269\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m--> 269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 6.63 GB, other allocations: 2.38 GB, max allowed: 9.07 GB). Tried to allocate 128.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs=8)\n",
    "trainer.fit(module, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eoTAB1fSOuk"
   },
   "source": [
    "### –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–¥–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4M_BAiMNl1rL"
   },
   "outputs": [],
   "source": [
    "def evaluate_task(model, test_dataloader):\n",
    "    model = model\n",
    "    model.eval()\n",
    "    taccuracy = 0.0\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images, labels = images, labels\n",
    "        with torch.no_grad():\n",
    "            #loss = model(images, labels)\n",
    "            output = model(images)\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            targets = labels\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            preds = pred\n",
    "            acc_batch = accuracy(preds.long(), targets.long(), task='multiclass', num_classes=200)\n",
    "        taccuracy += acc_batch\n",
    "    taccuracy = taccuracy / len(test_dataloader)\n",
    "    return taccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TsP57VG8KEfP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:33<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç—Ç–æ –∑–∞–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–∏—Ç 0.75 –±–∞–ª–ª–æ–≤\n",
      "tensor(0.0329)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = evaluate_task(model, val_dataloader)\n",
    "print(f\"–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç—Ç–æ –∑–∞–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–∏—Ç {np.clip(10 * accuracy_score / 0.44, 0, 10):.2f} –±–∞–ª–ª–æ–≤\")\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in tqdm(val_dataloader):\n",
    "        print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ –º–æ–¥–µ–ª–∏ ResNet50-200 —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ –æ–±—É—á–∞—é—Ç—Å—è. –¢–æ –∂–µ –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å –ø—Ä–æ –º–µ–Ω—å—à–∏–π batch_size, –Ω–∞–ø—Ä–∏–º–µ—Ä 64, 128, –∞ —Ç–∞–∫–∂–µ –∏–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (AdamW). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZqSdlQQKukS"
   },
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 2\n",
    "\n",
    "5 –±–∞–ª–ª–æ–≤\n",
    "–î–æ–±–µ–π—Ç–µ—Å—å accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.84. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –¥–µ–ª–∞—Ç—å —Ä–µ—Å–∞–π–∑ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ—Ç—Ä–µ–π–Ω –º–æ–∂–Ω–æ.\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤—ã–±–∏—Ç—å —Å–∫–æ—Ä (—Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–∏–∂–µ) –Ω–∞ 2.5/5 –±–∞–ª–ª–∞ (—Ç–æ –µ—Å—Ç—å –ø–æ–ª–æ–≤–∏–Ω—É –∑–∞ –∑–∞–¥–∞–Ω–∏–µ) –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø–∞—Ä—É –ø—Ä–æ—Å—Ç—ã—Ö –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª:\n",
    "1. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–±–µ–∑ –Ω–µ–µ —Å–ª–æ–∂–Ω–æ –æ—á–µ–Ω—å –±—É–¥–µ—Ç)\n",
    "2. –û–ø—Ç–∏–º–∞–π–∑–µ—Ä—ã –º–æ–∂–Ω–æ (–∏ –Ω—É–∂–Ω–æ) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –û–¥–Ω–∞–∫–æ –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç–µ, —Ç–æ –Ω–µ –º–µ–Ω—è–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ä–∞–∑—É - —Å–æ–±—å–µ—Ç–µ –ª–æ–≥–∏–∫—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "3. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Å–∞–º—ã–µ –ø–µ—Ä–≤—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (—á—Ç–æ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ö –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –∏–ª–∏ –º–æ–∂–µ—Ç–µ –ø–æ–π—Ç–∏ –¥–∞–ª—å—à–µ).\n",
    "4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫–∞–∫ baseline. –û—Ç—Å—é–¥–∞ –ø–æ–π–º–µ—Ç–µ –∫–∞–∫–∏–µ —Å–ª–æ–∏ –Ω—É–∂–Ω–æ –¥–æ–æ–±—É—á–∞—Ç—å.\n",
    "5. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ –Ω–æ—É—Ç–±—É–∫–∏ –ø—Ä–æ—à–µ–¥—à–∏—Ö —Å–µ–º–∏–Ω–∞—Ä–æ–≤ –∏ —Å–ª–µ–ø–∏—Ç—å –∏–∑ –Ω–∏—Ö —á—Ç–æ-—Ç–æ –æ–±—â–µ–µ. –°–µ–º–∏–Ω–∞—Ä—Å–∫–∏—Ö —Ç–µ—Ç—Ä–∞–¥–æ–∫ —Ö–≤–∞—Ç–∏—Ç —Å–≤–µ—Ä—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('/Users/nikitaevseev/Desktop/DS/DL/dataset/dataset/train', transform=transform_img)\n",
    "\n",
    "train_dataset_loader = DataLoader(\n",
    "  train_dataset, \n",
    "  batch_size=len(train_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomApply(transforms=[transforms.ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1)], p=0.2),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomApply(transforms=[transforms.AugMix()], p=0.25),\n",
    "        transforms.RandomApply(transforms=[transforms.Grayscale(num_output_channels=3)], p=0.07),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2770, 0.2691, 0.2821])\n",
    "    ]\n",
    ")\n",
    "#(0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)                    [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2770, 0.2691, 0.2821])\n",
    "    ]\n",
    ")\n",
    "\n",
    "#applier = transforms.RandomApply(transforms=[AugMix], p=0.5)\n",
    "\n",
    "#transforms = transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(), ]), p=0.3)\n",
    "\n",
    "train_dataset = ImageFolder('/Users/nikitaevseev/Desktop/DS/DL/dataset/dataset/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('/Users/nikitaevseev/Desktop/DS/DL/dataset/dataset/val', transform=val_transform)\n",
    "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=7, persistent_workers=True, multiprocessing_context='forkserver')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=7, persistent_workers=True, multiprocessing_context='forkserver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDzXM5rNxNQp"
   },
   "source": [
    "### –ú–æ–¥–µ–ª—å (–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDr9l76AxH_9"
   },
   "outputs": [],
   "source": [
    "class YourNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # runs the Neural Network\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def forward(self, images, target=None):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def get_accuracy(self, reset=False):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Flwqk0YjxPLE"
   },
   "source": [
    "### –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∫–ª–∞—Å—Å lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UnKHluO6xID4"
   },
   "outputs": [],
   "source": [
    "class YourModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4, wd=10.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wd = wd\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.model(x)\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate*10)\n",
    "        optimizer = Lion(self.parameters(), lr=self.learning_rate, weight_decay=self.wd)\n",
    "        return {\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": {\n",
    "            \"scheduler\": ReduceLROnPlateau(optimizer, factor=0.1),\n",
    "            \"monitor\": \"train_loss\",\n",
    "            \"frequency\": 1\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        images, target = train_batch\n",
    "        output = self.model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, prog_bar=True\n",
    "        )  # —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥–∏ –≤ –ø–∞–ø–∫—É, –Ω–æ –º–æ–∂–Ω–æ –Ω–µ—Å–ª–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å wandb\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        images, target = val_batch\n",
    "        #loss = self.model(images, target)\n",
    "        output = self.model(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lQAjkvkVyhEg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaevseev/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikitaevseev/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nikitaevseev/Desktop/DS/DL/wandb/run-20231103_151028-rgnpg0i3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/rgnpg0i3' target=\"_blank\">vit_h_14+Lion+entr+512</a></strong> to <a href='https://wandb.ai/naevseev-work/hw_02_exs1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naevseev-work/hw_02_exs1' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naevseev-work/hw_02_exs1/runs/rgnpg0i3' target=\"_blank\">https://wandb.ai/naevseev-work/hw_02_exs1/runs/rgnpg0i3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger\\\n",
    "    (log_model='all') # –∫–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–∞–µ—Ç —Å pytorch_lightning https://docs.wandb.ai/guides/integrations/lightning\n",
    "\n",
    "model = torchvision.models.vit_h_14(pretrained=False, num_classes=200)#YourNet(dropout=0.4)\n",
    "module = YourModule(model) # YOUR CODE HERE\n",
    "\n",
    "# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç\n",
    "wandb.init(project=\"hw_02_exs1\", name=\"vit_h_14+Lion+entr+512\")\n",
    "# —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∫–∏ –≤ wandb + –ø—Ä–æ—Å–∏–º —Å–ª–µ–¥–∏—Ç—å –∑–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ —Å–µ—Ç–∫–∏\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4wJgB1YyhG0"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs=2)\n",
    "trainer.fit(module, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5D2bwOKSHVp"
   },
   "source": [
    "### –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–¥–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEdwJE5uOrIM"
   },
   "outputs": [],
   "source": [
    "model = # –ü–æ–¥–≥—Ä—É–∑–∏—Ç—å —Å–≤–æ—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å—é–¥–∞\n",
    "\n",
    "accuracy = evaluate_task(model, val_dataloader)\n",
    "print(f\"–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç—Ç–æ –∑–∞–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–∏—Ç {np.clip(10 * (accuracy - 0.5) / 0.34, 0, 10):.2f} –±–∞–ª–ª–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 15,
    "id": "pT8vfPSolRVb"
   },
   "source": [
    "# –û—Ç—á—ë—Ç –æ–± —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö \n",
    "\n",
    "—Ç–µ–∫—Å—Ç –ø–∏—Å–∞—Ç—å —Ç—É—Ç (–∏–ª–∏ —Å—Å—ã–ª–æ—á–∫—É –Ω–∞ wandb/–ª—é–±–æ–π —Ç—Ä–µ–∫–µ—Ä —ç–∫—Å–ø—Ä–µ–∏–º–µ–Ω—Ç–æ–≤) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è, —Ç–æ –µ—Å—Ç—å –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏–º–µ–Ω–Ω–æ —Ç—É—Ç —Ä–∏—Å–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏, –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –≥–æ—Ç–æ–≤—ã–µ —Ç—Ä–µ–∫–µ—Ä—ã/–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∏ –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkGZ3kuULB55"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "max_cell_id": 35
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
